{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "humanitarian-capability",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "Clustering is a family of unsupervised ML techniques that group heterogeneous instances into homogenous sub-groups. Unlike classification, where we train the model with given labels, the groups are not known before and depend on the data as well as on our hyperparameter settings.\n",
    "\n",
    "Clustering is widely used for things like customer segmentation, dimensionality reduction, anomaly detection, search engine optimization, image segmentation in object detection problems, and so on.\n",
    "\n",
    "## 1. K-Means\n",
    "The K-Means is a very simple but capable clustering algorithm that very efficiently and quickly separates data into sub-groups after only a few iterations. We define k as the number of clusters we want to have in the end. \n",
    "\n",
    "The algorithm starts by randomly assigning k points as starting centroids and group the other training instances by distance to these points. We have k groups now and the centroids are updated as there might be a new mean point within the temporary clusters. This process is repeated until new centroids cannot be calculated anymore, i.e. the distance cannot be reduced.\n",
    "\n",
    "The distance is called inertia, which by default is the mean squared distance between each instance and the closes centroid.\n",
    "\n",
    "**Initialization:** Since this process of centroid initialization is random, K-Means is stochastic and the resulting clusters can be different after each run of k-Means, even leading to sub-optimal solutions by chance. The initial centroids can be defined using the init hyperparameter if we have a good feeling for where the groups should be. Another way proposed by K-Means++ where the initial centroids are not chosen completely randomly but should be distant from another. This is what Scikit-Learn's k-Means uses by default.\n",
    "\n",
    "**K-Means variants and speed:** The algorithm can be sped up by avoiding unnecessary distance calculations, which scikit-learn's k-Means implementation uses by default. Another way of speeding up the process is to not use the entire dataset at each iteration but mini-batches (sklearn MiniBatchKMeans). This typically leads to a speed improvement of three or four times and allows to deal with huge data sets that do not fit in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "charitable-kingdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-eight",
   "metadata": {},
   "source": [
    "**k: optimal number of clusters:** The choice of k determines how good the model is and is thus not an easy decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-firewall",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
