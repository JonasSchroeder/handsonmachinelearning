{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "authentic-combat",
   "metadata": {},
   "source": [
    "# Training Deep Neural Networks (DNNs)\n",
    "To tackle complex problems we often need deeper models with 10+ hidde layers, hundreds of neurons per layer, linked by hundreds of thousands of connections.This can cause some common challenges:\n",
    "- Vanishing Gradients Problem -> many solutions below\n",
    "- Exploding Gradients Problem -> many solutions below\n",
    "- Lack of Training Data and Overfitting Noisy Data -> transfer learning and unsupervised pretraining\n",
    "- Slow Training -> choice of optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civil-frontier",
   "metadata": {},
   "source": [
    "# 1. Vanishing and Exploding Gradients\n",
    "The backpropagation algorithm works by going from the output layer to the input layer and computing the gradient of the cost function with regard to each parameter of the network, then using these gradients to update the parameter with Gradient Descent.\n",
    "\n",
    "As the algorithm progresses backwards, gradients often get smaller towards the earlier layers and Gradient Descent barely updates these weights. The reason is that chain rule is used to calculate the gradients, multiplying a bunch of small values <1 within the formula, and thus make the overall results small as well.\n",
    "\n",
    "Since the weight update is proportional to the gradient size, the update of early layer weights is very small, which hinders learning and training never converges to a good solution. This is called the **vanishing gradients problem**. \n",
    "\n",
    "In some cases, the opposite can happen and gradients become larger and larger, leading to overadjust the weights in the earlier layers of the network. We would move further and further away from the optimal weights and the algorithm diverges. This problem is called **exploding gradients** and is quite common for recurrent neural networks (RNNs).\n",
    "\n",
    "Researchers in 2010 found the combination of logistic sigmoid activation function and a normal-distribution weight initialization technique to be a common reason for this behavior. The result is that the variance of each layer's output is larger than for its input.\n",
    "\n",
    "#### Choice of Initialization\n",
    "Glorot and He propose an initialization techniques where input and output variance stays equal when moving forward (like a chain of microphones and amplifiers), and the variance of the gradients to stay equal before and after moving through the layers in backpropagation. Both cannot be guaranteed but the **Xavier or Glorot Initialization** found a good compromise that works well in practice. It is very similar to LeCun Initialization (1990s).\n",
    "\n",
    "Keras uses Glorot initialization with a uniform distribution by default. This behavior can be changed using kernal_initialization.\n",
    "\n",
    "#### Nonsaturating Activation Functions\n",
    "Next to initialization the choice of activation functions can lead to saturations. ReLu is great in general since it does not saturate for positive values and is fast to compute. However, some neurons can die turing training, meaning they stop outputting anything other than 0. This problem is known as **dying ReLU**. \n",
    "\n",
    "The variant called **LeakyReLU** avoids this problem by having a non-zero slope even for negative values. There are many types of such leaky ReLus and we can influence the leakiness by tweaking the hyperparameter alpha. \n",
    "\n",
    "Alternatively, we can use the **ELU function** which looks like a smoothed ReLu. Usually, ELU leads to longer training time than ReLu. There is a variant called SELU which can lead to the network's self-normalization given certain prerequesits.\n",
    "\n",
    "Geron suggests: SELU > ELU > Leaky ReLU > ReLU > tanh > logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-delhi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeakyReLU\n",
    "model = keras.models.Sequential([\n",
    "    [...],\n",
    "    keras.layers.Dense(30, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(alpha=0.2)\n",
    "    [...]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specialized-singer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELU\n",
    "model = keras.models.Sequential([\n",
    "    [...],\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    [...]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-metadata",
   "metadata": {},
   "source": [
    "### Batch Normalization\n",
    "Proper initialization prevents the vanishing/exploding gradients problem in the beginning of training the network. However, it can occur later. **Batch Normalization (BN)** addresses this problem. \n",
    "\n",
    "An operation is added just before and after the activation function of each hidden layer that zero-centers and normalizes each input, then scales and shifts the result. Thus, the operation leds the model learn the optimal scale and mean of each of the layer's input.\n",
    "\n",
    "Adding a BN layer as the very first layer is roughly equivalent to standardizing the data before training. \n",
    "\n",
    "Four parameter vectors are learning in keras' standard implementation of Batch Normalization: \n",
    "- the output scale vector (backpropagation)\n",
    "- the output offset vector (backpropagation)\n",
    "- the final input mean vector (estimated during training but used only after training)\n",
    "- the final input standard deviation vector (estimated during training but used only after training)\n",
    "\n",
    "Researchers generally found **Batch Normalization to improve all kinds of DNNs**,, e.g. lead to huge improvements on the ImageNet classification task. Using more saturating activation functions like tanh and logistic activation function become possible again. Furthermore, BN acts as a regulizer, reducing the need for dropouts.\n",
    "\n",
    "Downsides are slower predictions and slightly more complex models. Epochs are generally found to be slower, however, fewer epochs are usually needed for convergence. Thus, wall time will usually be shorter.\n",
    "\n",
    "Applying BN after each layer is so common in practice that the BN layers are hidden in diagrams. New bleeding-edge research challenges this approach but as of today it is still good practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-glucose",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BN as first layer and after every hidden layer\n",
    "# usually necessary for way deeper networks\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28])\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation_function=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-federal",
   "metadata": {},
   "source": [
    "### Gradient Clipping\n",
    "Another approach to avoid exploding gradients is to clip gradients during backpropagation by using a threshold. This approach is an alternative to BN in RNNs, for other types BN is sufficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "artificial-regulation",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-494a53b19305>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclipvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# orientation of the gradient might change; instead clipnorm=1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mse\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.SGD(clipvalue=1.0) # orientation of the gradient might change; instead clipnorm=1.0\n",
    "model.compile(loss=\"mse\", optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seeing-might",
   "metadata": {},
   "source": [
    "# 2. Reusing Pretrained Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extreme-niger",
   "metadata": {},
   "source": [
    "Chapter 14 shows ways of finding a good existing neural network. We shouldn't train a deep network from scratch but instead reuse the lower layers of an existing network. This technique is called **transfer learning**.\n",
    "\n",
    "For example: We want to use Model A, which was used for a similar task, and extend it for our current task. We load the model, remove the last layer, and replace it by a new one that suits the problem (e.g., binary classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984dcf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A = keras.models.load_model(\"my_model_A.h5\")\n",
    "model_B_on_A = keras.models.Sequential(model_A[:-1])\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2d6948",
   "metadata": {},
   "source": [
    "Training the new model will change the weights of the old model. We want to avoid that and use the results from pretraining. Therefore we need to clone model A first and copy the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c100e9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863ea5ba",
   "metadata": {},
   "source": [
    "Training the new model on the new task can lead to large error gradients that could destroy the reused weights. Therefore it is adviced to freeze the reused layers during the first few epochs. \n",
    "\n",
    "Note: Freezing and unfreezing requires to compile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f5f4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\", optimize=\"sgd\",\n",
    "                    metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2182345",
   "metadata": {},
   "source": [
    "We can then train the new model for a few epochs, change trainability of layers back to true, and continue. A good approach is to reduce the learning rate after unfreezing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86cdcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,\n",
    "                          validation_data=(X_valid_B, y_valid_B))\n",
    "\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "\n",
    "optimizer = keras.optimizers.SGD(lr=0.0001)\n",
    "\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\", optimize=\"sgd\",\n",
    "                    metrics=[\"accuracy\"])\n",
    "\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\n",
    "                          validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177a07e4",
   "metadata": {},
   "source": [
    "Note: Transfer learning does not work with small dense networks but best with deep CNNs (Chapter 14)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a018f6",
   "metadata": {},
   "source": [
    "# 3. Faster Optimizers\n",
    "\n",
    "To summarize, speeding up training can be achieved by the using:\n",
    "- good initialization strategy for the connection weights\n",
    "- good activation function\n",
    "- Batch Normalization\n",
    "- reusing parts from pre-trained models\n",
    "\n",
    "Additionally, we can speed training up by using faster optimizers than regular SGD.\n",
    "- momentum optimization\n",
    "- Nesterov Accelerated Gradient (NAG)\n",
    "- AdaGrad\n",
    "- RMSProp\n",
    "- Adam and Nadam (adam+nag) optimization\n",
    "\n",
    "Advantages and Disadvantages on p.359"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ade5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# momentum optimizer\n",
    "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)\n",
    "\n",
    "# NAG\n",
    "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)\n",
    "\n",
    "# RMSprop\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9) # replaced by Adam optimizers\n",
    "\n",
    "# Adam\n",
    "optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313946c1",
   "metadata": {},
   "source": [
    "# 4. Learning Rate Scheduling\n",
    "\n",
    "A common starting approach when searching for a good learning rate is to start very small, train for a few hundred iterations, exponentially increase lr, repeat to very large lr values, then plot the loss as a function of the learning rate, and choose the lr just before the loss curve shoots up again.\n",
    "\n",
    "This approach uses a constant learning rate. We could reach a good solution faster by using a dynamic learning rate, for exampple bigger in the beginning and smaller later. There are many such strategies calles **learning rate scheduling**.\n",
    "\n",
    "- power scheduling\n",
    "- exponential scheduling\n",
    "- piecewise constant scheduling\n",
    "- performance scheduling\n",
    "- icycle scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60bbcf4",
   "metadata": {},
   "source": [
    "# 5. Avoiding Overfitting through Regularization\n",
    "- L1 and L2 regularization\n",
    "- Dropout\n",
    "- Monte Carlo (MC) Dropout\n",
    "- Max-Norm regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cea4951",
   "metadata": {},
   "source": [
    "# 6. Practical Guidelines\n",
    "\n",
    "Default DNN configuration:\n",
    "\n",
    "    Kernel initializer: He initialization\n",
    "    Activation function: ELU\n",
    "    Normalization: None if shallow, Batch Norm if deep\n",
    "    Regularization: Early stopping +L2 if needed\n",
    "    Optimizer: Momentum optimization like Nadam or RMSProp\n",
    "    Learing rate scheduler: 1cycle\n",
    "\n",
    "Don't forget to normalize the input features and to use parts of pre-trained models if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4287d4f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
