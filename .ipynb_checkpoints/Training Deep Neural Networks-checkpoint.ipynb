{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "permanent-radius",
   "metadata": {},
   "source": [
    "# Training Deep Neural Networks (DNNs)\n",
    "To tackle complex problems we often need deeper models with 10+ hidde layers, hundreds of neurons per layer, linked by hundreds of thousands of connections.This can cause some common challenges:\n",
    "- Vanishing Gradients Problem -> many solutions below\n",
    "- Exploding Gradients Problem -> many solutions below\n",
    "- Lack of Training Data and Overfitting Noisy Data -> transfer learning and unsupervised pretraining\n",
    "- Slow Training -> choice of optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-cattle",
   "metadata": {},
   "source": [
    "## Vanishing and Exploding Gradients\n",
    "The backpropagation algorithm works by going from the output layer to the input layer and computing the gradient of the cost function with regard to each parameter of the network, then using these gradients to update the parameter with Gradient Descent.\n",
    "\n",
    "As the algorithm progresses, gradients often get smaller down to the lower layers and Gradient Descent barely updates the weights of lower layers. Training never converges to a good solution. This is called the **vanishing gradients problem**. In some cases, the opposite can happen and gradients become larger and larger, leading the algorithm to diverge. This problem is called **exploding gradients** and is quite common for recurrent neural networks (RNNs).\n",
    "\n",
    "Researchers in 2010 found the combination of logistic sigmoid activation function and a normal-distribution weight initialization technique to be a common reason for this behavior. The result is that the variance of each layer's output is larger than for its input.\n",
    "\n",
    "#### Choice of Initialization\n",
    "Glorot and He propose an initialization techniques where input and output variance stays equal when moving forward (like a chain of microphones and amplifiers), and the variance of the gradients to stay equal before and after moving through the layers in backpropagation. Both cannot be guaranteed but the **Xavier or Glorot Initialization** found a good compromise that works well in practice. It is very similar to LeCun Initialization (1990s).\n",
    "\n",
    "Keras uses Glorot initialization with a uniform distribution by default. This behavior can be changed using kernal_initialization.\n",
    "\n",
    "#### Nonsaturating Activation Functions\n",
    "Next to initialization the choice of activation functions can lead to saturations. ReLu is great in general since it does not saturate for positive values and is fast to compute. However, some neurons can die turing training, meaning they stop outputting anything other than 0. This problem is known as **dying ReLU**. \n",
    "\n",
    "The variant called **LeakyReLU** avoids this problem by having a non-zero slope even for negative values. There are many types of such leaky ReLus and we can influence the leakiness by tweaking the hyperparameter alpha. \n",
    "\n",
    "Alternatively, we can use the **ELU function** which looks like a smoothed ReLu. Usually, ELU leads to longer training time than ReLu. There is a variant called SELU which can lead to the network's self-normalization given certain prerequesits.\n",
    "\n",
    "Geron suggests: SELU > ELU > Leaky ReLU > ReLU > tanh > logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-saskatchewan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeakyReLU\n",
    "model = keras.models.Sequential([\n",
    "    [...],\n",
    "    keras.layers.Dense(30, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(alpha=0.2)\n",
    "    [...]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-romance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELU\n",
    "model = keras.models.Sequential([\n",
    "    [...],\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    [...]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-ranch",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "Proper initialization prevents the vanishing/exploding gradients problem in the beginning of training the network. However, it can occur later. **Batch Normalization (BN)** addresses this problem. \n",
    "\n",
    "An operation is added just before and after the activation function of each hidden layer that zero-centers and normalizes each input, then scales and shifts the result. Thus, the operation leds the model learn the optimal scale and mean of each of the layer's input.\n",
    "\n",
    "Adding a BN layer as the very first layer is roughly equivalent to standardizing the data before training. \n",
    "\n",
    "Four parameter vectors are learning in keras' standard implementation of Batch Normalization: \n",
    "- the output scale vector (backpropagation)\n",
    "- the output offset vector (backpropagation)\n",
    "- the final input mean vector (estimated during training but used only after training)\n",
    "- the final input standard deviation vector (estimated during training but used only after training)\n",
    "\n",
    "Researchers generally found **Batch Normalization to improve all kinds of DNNs**,, e.g. lead to huge improvements on the ImageNet classification task. Using more saturating activation functions like tanh and logistic activation function become possible again. Furthermore, BN acts as a regulizer, reducing the need for dropouts.\n",
    "\n",
    "Downsides are slower predictions and slightly more complex models. Epochs are generally found to be slower, however, fewer epochs are usually needed for convergence. Thus, wall time will usually be shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive-elizabeth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BN as first layer and after every hidden layer\n",
    "# usually necessary for way deeper networks\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28])\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation_function=\"softmax\")\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
