{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intense-spoke",
   "metadata": {},
   "source": [
    "# Intro to ANNs\n",
    "\n",
    "Artificial Neural Networks has been around for a while with varying interest. NNs regularly outperform other ML techniques on very large and complex problems. Experts expect NNs to stay popular this time due to the vast amount of available data and compute power. Although this all sounds good, lack of explainability (black box model) could become more of an issue with AI ethics.\n",
    "\n",
    "## 1. Perceptron\n",
    "\n",
    "Invented in 1957, the Perceptron is one of the simplest ANN architecture. It consists of just one **threshold logic unit (TLU)**, which computes the weighted sum of the inputs and applies a step function (as activation function) to that sum to generate outputs. \n",
    "\n",
    "Thus, the Perceptron with a single TLU can be used for simple linear binary classification similar to Logistic Regression. Once the weighted sum exceeds the threshold, the instance is considered to be positive, else negative. Extending the Perceptron with multiple TLUs allows for multiclass predictions.\n",
    "\n",
    "A big downside of Perceptrons is that they are incapable of learning complex patterns since the decision boundary is linear (same as in Logistic Regression). A more complex ANN is necesseray for linearily non-separable datasets. Furthermore, compared to Logistic regression the Perceptron does not output class probabiity but class belonging based on a hard threshold, which is why Logistic Regression should be preferred.\n",
    "\n",
    "**Let's apply the Perceptron with a single TLU to the iris dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "disturbed-spokesman",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris[\"data\"][:,2:3] # Petal length and width\n",
    "y = (iris[\"target\"]==0) # Setosa?\n",
    "y = y.astype(np.int)\n",
    "\n",
    "per_clf = Perceptron()\n",
    "per_clf.fit(X, y)\n",
    "\n",
    "y_pred = per_clf.predict(X)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "suitable-mission",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-marker",
   "metadata": {},
   "source": [
    "## 2. Multilayer Perceptron (MLP)\n",
    "A single TLU Perceptron is very limited. Some if its limitations can be eliminated by stacking multiple Perceptrons together as multiple layers, resulting in a network architecture called **Multilayer Perceptron (MLP)**.\n",
    "\n",
    "An MLP is composed of one input layer followed by multiple layers of TLUs (hidden layers) and a final layer of TLUs (output layer). Every layer except the output layer has a bias unit. Each layer is fully connected. \n",
    "\n",
    "One key difference is that instead of a step function, MLPs use a **sigmoid function** as activation function. Otherwise, the flat surface of a stepfunction would make it impossible to calculate gradients, and the Gradient Descent algorithm for optimization would not worked (other functions would work too, like ReLu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "built-manner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual classes: [1 0 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1]\n",
      "predicted classes: [1 0 1 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1]\n",
      "accuracy score: 0.92\n"
     ]
    }
   ],
   "source": [
    "# Perceptron (single TLU)\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_classification(n_samples=100, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=1)\n",
    "\n",
    "perceptron = Perceptron(random_state=1, max_iter=300).fit(X_train, y_train)\n",
    "\n",
    "print(\"actual classes: \" + str(y_test))\n",
    "print(\"predicted classes: \" + str(perceptron.predict(X_test)))\n",
    "print(\"accuracy score: \" + str(perceptron.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "raising-discovery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual classes: [1 0 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1]\n",
      "predicted classes: [1 0 1 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 1 1 0 0 1]\n",
      "accuracy score: 0.88\n",
      "number of layers: 3\n"
     ]
    }
   ],
   "source": [
    "# Multilayer Perceptron Classifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_classification(n_samples=100, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=1)\n",
    "\n",
    "mlp_clf = MLPClassifier(random_state=1, max_iter=300).fit(X_train, y_train)\n",
    "\n",
    "print(\"actual classes: \" + str(y_test))\n",
    "print(\"predicted classes: \" + str(mlp_clf.predict(X_test)))\n",
    "#print(mlp_clf.predict_proba(X_test).ravel())\n",
    "print(\"accuracy score: \" + str(mlp_clf.score(X_test, y_test)))\n",
    "print(\"number of layers: \" + str(mlp_clf.n_layers_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removable-survivor",
   "metadata": {},
   "source": [
    "The **Backpropagation** algorithm is essentially calculating gradient descent forward and backwards, thus finding the network error with regard to every single model parameter. Knowing the error per parameter setting let's us tweak the weights in order to reduce the error. This is repeated until the network converges to a solution.\n",
    "\n",
    "Note: Automatically computing gradients is called **automatic differentiation (autodiff)**. Backpropagation is a type of autodiff, called reverse-mode autodiff.\n",
    "\n",
    "### Learning process:\n",
    "\n",
    "- We start with random weights (initialization).\n",
    "\n",
    "- Using mini-batches, the algorithm goes through the full training set multiple times. Each passthrough is called an **epoch**. \n",
    "\n",
    "- The result of one layer becomes the input of the next layer until we reach the final layer (forward pass). \n",
    "\n",
    "- Then the output is compared to the desired output and a measure of error is returned. \n",
    "\n",
    "- Next, it calculates how much each output connection contributed to the error, and how much of the error came from the connections in the layer before, and so on (reverse pass). \n",
    "\n",
    "- Finally, Gradient Descent is performed to tweak all the connction weights in the network using the error gradients just computed.\n",
    "\n",
    "- All this together is called Backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-palace",
   "metadata": {},
   "source": [
    "## 2.1 MLP for Regression\n",
    "\n",
    "To predict a single value like housing prices, we need one output layer (one output layer per output dimension). We won't need an activation function before the output layer if we don't want to limit the value range. In case we want to make sure the output is only positive, we can use the ReLu function. Sigmoid function and scaling should be used when the value should be bound to a certain range. MSE or MAE are standard loss functions for MLP regressions.\n",
    "\n",
    "\n",
    "## 2.2 MLP for Classification\n",
    "\n",
    "A binary classification problem can be solved with a single output neuron and a sigmoid activation function. The resulting number corresponds to the estimated probabibility of belonging to the positive class. In multiclass problems we need one output neuron per class and use softmax as activation function. A standard loss function is the cross-entropy loss (or log loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "corresponding-canon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.4.1-cp38-cp38-macosx_10_11_x86_64.whl (173.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 173.9 MB 4.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions~=3.7.4 in /Users/jonasschroeder/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: six~=1.15.0 in /Users/jonasschroeder/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /Users/jonasschroeder/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (0.36.2)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /Users/jonasschroeder/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.19.2)\n",
      "Requirement already satisfied: h5py~=2.10.0 in /Users/jonasschroeder/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (2.10.0)\n",
      "Collecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting absl-py~=0.10\n",
      "  Downloading absl_py-0.11.0-py3-none-any.whl (127 kB)\n",
      "\u001b[K     |████████████████████████████████| 127 kB 5.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting google-pasta~=0.2\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 5.7 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting grpcio~=1.32.0\n",
      "  Downloading grpcio-1.32.0-cp38-cp38-macosx_10_9_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 5.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 3.3 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 5.2 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.15.5-cp38-cp38-macosx_10_9_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 5.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard~=2.4\n",
      "  Downloading tensorboard-2.4.1-py3-none-any.whl (10.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.6 MB 5.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /Users/jonasschroeder/opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/jonasschroeder/opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (51.3.3.post20210118)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/jonasschroeder/opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (2.25.1)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.27.1-py2.py3-none-any.whl (136 kB)\n",
      "\u001b[K     |████████████████████████████████| 136 kB 4.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.1-py3-none-any.whl (12 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 5.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 4.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jonasschroeder/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/jonasschroeder/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/jonasschroeder/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/jonasschroeder/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.26.2)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 5.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.5.0,>=2.4.0\n",
      "  Downloading tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 4.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor~=1.1.0\n",
      "  Using cached termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting wrapt~=1.12.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Building wheels for collected packages: termcolor, wrapt\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=0b0baf5619f6ba42dba6e535cc12e35c66fb7597161aaca1218bde0913cf0ce0\n",
      "  Stored in directory: /Users/jonasschroeder/Library/Caches/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-cp38-cp38-macosx_10_9_x86_64.whl size=32651 sha256=f298b8daffa03ee306bab64b443a442e23897e8bccf6f435451903d6733c9304\n",
      "  Stored in directory: /Users/jonasschroeder/Library/Caches/pip/wheels/5f/fd/9e/b6cf5890494cb8ef0b5eaff72e5d55a70fb56316007d6dfe73\n",
      "Successfully built termcolor wrapt\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, tensorboard-plugin-wit, protobuf, markdown, grpcio, google-auth-oauthlib, absl-py, wrapt, termcolor, tensorflow-estimator, tensorboard, opt-einsum, keras-preprocessing, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.11.2\n",
      "    Uninstalling wrapt-1.11.2:\n",
      "      Successfully uninstalled wrapt-1.11.2\n",
      "Successfully installed absl-py-0.11.0 astunparse-1.6.3 cachetools-4.2.1 flatbuffers-1.12 gast-0.3.3 google-auth-1.27.1 google-auth-oauthlib-0.4.3 google-pasta-0.2.0 grpcio-1.32.0 keras-preprocessing-1.1.2 markdown-3.3.4 oauthlib-3.1.0 opt-einsum-3.3.0 protobuf-3.15.5 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.7.2 tensorboard-2.4.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.4.1 tensorflow-estimator-2.4.0 termcolor-1.1.0 wrapt-1.12.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accompanied-polyester",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
