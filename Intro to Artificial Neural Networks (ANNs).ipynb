{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "particular-granny",
   "metadata": {},
   "source": [
    "# Intro to ANNs\n",
    "\n",
    "Artificial Neural Networks has been around for a while with varying interest. NNs regularly outperform other ML techniques on very large and complex problems. Experts expect NNs to stay popular this time due to the vast amount of available data and compute power. Although this all sounds good, lack of explainability (black box model) could become more of an issue with AI ethics.\n",
    "\n",
    "## 1. Perceptron\n",
    "\n",
    "Invented in 1957, the Perceptron is one of the simplest ANN architecture. It consists of just one **threshold logic unit (TLU)**, which computes the weighted sum of the inputs and applies a step function (as activation function) to that sum to generate outputs. \n",
    "\n",
    "Thus, the Perceptron with a single TLU can be used for simple linear binary classification similar to Logistic Regression. Once the weighted sum exceeds the threshold, the instance is considered to be positive, else negative. Extending the Perceptron with multiple TLUs allows for multiclass predictions.\n",
    "\n",
    "A big downside of Perceptrons is that they are incapable of learning complex patterns since the decision boundary is linear (same as in Logistic Regression). A more complex ANN is necesseray for linearily non-separable datasets. Furthermore, compared to Logistic regression the Perceptron does not output class probabiity but class belonging based on a hard threshold, which is why Logistic Regression should be preferred.\n",
    "\n",
    "**Let's apply the Perceptron with a single TLU to the iris dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "suspended-discount",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris[\"data\"][:,2:3] # Petal length and width\n",
    "y = (iris[\"target\"]==0) # Setosa?\n",
    "y = y.astype(np.int)\n",
    "\n",
    "per_clf = Perceptron()\n",
    "per_clf.fit(X, y)\n",
    "\n",
    "y_pred = per_clf.predict(X)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "greek-rwanda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-concord",
   "metadata": {},
   "source": [
    "### 2. Multilayer Perceptron\n",
    "A single TLU Perceptron is very limited. Some if its limitations can be eliminated by stacking multiple Perceptrons together as multiple layers, resulting in a network architecture called **Multilayer Perceptron (MLP)**.\n",
    "\n",
    "An MLP is composed of one input layer followed by multiple layers of TLUs (hidden layers) and a final layer of TLUs (output layer). Every layer except the output layer has a bias unit. Each layer is fully connected. \n",
    "\n",
    "One key difference is that instead of a step function, MLPs use a **sigmoid function** as activation function. Otherwise, the flat surface of a stepfunction would make it impossible to calculate gradients, and the Gradient Descent algorithm for optimization would not worked (other functions would work too, like ReLu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "close-given",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual classes: [1 0 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1]\n",
      "predicted classes: [1 0 1 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1]\n",
      "accuracy score: 0.92\n"
     ]
    }
   ],
   "source": [
    "# Perceptron (single TLU)\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_classification(n_samples=100, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=1)\n",
    "\n",
    "perceptron = Perceptron(random_state=1, max_iter=300).fit(X_train, y_train)\n",
    "\n",
    "print(\"actual classes: \" + str(y_test))\n",
    "print(\"predicted classes: \" + str(perceptron.predict(X_test)))\n",
    "print(\"accuracy score: \" + str(perceptron.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "better-thursday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual classes: [1 0 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1]\n",
      "predicted classes: [1 0 1 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 1 1 0 0 1]\n",
      "accuracy score: 0.88\n",
      "number of layers: 3\n"
     ]
    }
   ],
   "source": [
    "# Multilayer Perceptron Classifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_classification(n_samples=100, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=1)\n",
    "\n",
    "mlp_clf = MLPClassifier(random_state=1, max_iter=300).fit(X_train, y_train)\n",
    "\n",
    "print(\"actual classes: \" + str(y_test))\n",
    "print(\"predicted classes: \" + str(mlp_clf.predict(X_test)))\n",
    "#print(mlp_clf.predict_proba(X_test).ravel())\n",
    "print(\"accuracy score: \" + str(mlp_clf.score(X_test, y_test)))\n",
    "print(\"number of layers: \" + str(mlp_clf.n_layers_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-gibraltar",
   "metadata": {},
   "source": [
    "The **Backpropagation** algorithm is essentially calculating gradient descent forward and backwards, thus finding the network error with regard to every single model parameter. Knowing the error per parameter setting let's us tweak the weights in order to reduce the error. This is repeated until the network converges to a solution.\n",
    "\n",
    "Note: Automatically computing gradients is called **automatic differentiation (autodiff)**. Backpropagation is a type of autodiff, called reverse-mode autodiff.\n",
    "\n",
    "### Learning process:\n",
    "\n",
    "We start with random weights (initialization).\n",
    "\n",
    "Using mini-batches, the algorithm goes through the full training set multiple times. Each passthrough is called an **epoch**. \n",
    "\n",
    "The result of one layer becomes the input of the next layer until we reach the final layer (forward pass). \n",
    "\n",
    "Then the output is compared to the desired output and a measure of error is returned. \n",
    "\n",
    "Next, it calculates how much each output connection contributed to the error, and how much of the error came from the connections in the layer before, and so on (reverse pass). \n",
    "\n",
    "Finally, Gradient Descent is performed to tweak all the connction weights in the network using the error gradients just computed.\n",
    "\n",
    "All this together is called Backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mechanical-madison",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
