{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "authentic-combat",
   "metadata": {},
   "source": [
    "# Training Deep Neural Networks (DNNs)\n",
    "To tackle complex problems we often need deeper models with 10+ hidde layers, hundreds of neurons per layer, linked by hundreds of thousands of connections.This can cause some common challenges:\n",
    "- Vanishing Gradients Problem -> many solutions below\n",
    "- Exploding Gradients Problem -> many solutions below\n",
    "- Lack of Training Data and Overfitting Noisy Data -> transfer learning and unsupervised pretraining\n",
    "- Slow Training -> choice of optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civil-frontier",
   "metadata": {},
   "source": [
    "# 1. Vanishing and Exploding Gradients\n",
    "The backpropagation algorithm works by going from the output layer to the input layer and computing the gradient of the cost function with regard to each parameter of the network, then using these gradients to update the parameter with Gradient Descent.\n",
    "\n",
    "As the algorithm progresses backwards, gradients often get smaller towards the earlier layers and Gradient Descent barely updates these weights. The reason is that chain rule is used to calculate the gradients, multiplying a bunch of small values <1 within the formula, and thus make the overall results small as well.\n",
    "\n",
    "Since the weight update is proportional to the gradient size, the update of early layer weights is very small, which hinders learning and training never converges to a good solution. This is called the **vanishing gradients problem**. \n",
    "\n",
    "In some cases, the opposite can happen and gradients become larger and larger, leading to overadjust the weights in the earlier layers of the network. We would move further and further away from the optimal weights and the algorithm diverges. This problem is called **exploding gradients** and is quite common for recurrent neural networks (RNNs).\n",
    "\n",
    "Researchers in 2010 found the combination of logistic sigmoid activation function and a normal-distribution weight initialization technique to be a common reason for this behavior. The result is that the variance of each layer's output is larger than for its input.\n",
    "\n",
    "#### Choice of Initialization\n",
    "Glorot and He propose an initialization techniques where input and output variance stays equal when moving forward (like a chain of microphones and amplifiers), and the variance of the gradients to stay equal before and after moving through the layers in backpropagation. Both cannot be guaranteed but the **Xavier or Glorot Initialization** found a good compromise that works well in practice. It is very similar to LeCun Initialization (1990s).\n",
    "\n",
    "Keras uses Glorot initialization with a uniform distribution by default. This behavior can be changed using kernal_initialization.\n",
    "\n",
    "#### Nonsaturating Activation Functions\n",
    "Next to initialization the choice of activation functions can lead to saturations. ReLu is great in general since it does not saturate for positive values and is fast to compute. However, some neurons can die turing training, meaning they stop outputting anything other than 0. This problem is known as **dying ReLU**. \n",
    "\n",
    "The variant called **LeakyReLU** avoids this problem by having a non-zero slope even for negative values. There are many types of such leaky ReLus and we can influence the leakiness by tweaking the hyperparameter alpha. \n",
    "\n",
    "Alternatively, we can use the **ELU function** which looks like a smoothed ReLu. Usually, ELU leads to longer training time than ReLu. There is a variant called SELU which can lead to the network's self-normalization given certain prerequesits.\n",
    "\n",
    "Geron suggests: SELU > ELU > Leaky ReLU > ReLU > tanh > logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-delhi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeakyReLU\n",
    "model = keras.models.Sequential([\n",
    "    [...],\n",
    "    keras.layers.Dense(30, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(alpha=0.2)\n",
    "    [...]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specialized-singer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELU\n",
    "model = keras.models.Sequential([\n",
    "    [...],\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    [...]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-metadata",
   "metadata": {},
   "source": [
    "### Batch Normalization\n",
    "Proper initialization prevents the vanishing/exploding gradients problem in the beginning of training the network. However, it can occur later. **Batch Normalization (BN)** addresses this problem. \n",
    "\n",
    "An operation is added just before and after the activation function of each hidden layer that zero-centers and normalizes each input, then scales and shifts the result. Thus, the operation leds the model learn the optimal scale and mean of each of the layer's input.\n",
    "\n",
    "Adding a BN layer as the very first layer is roughly equivalent to standardizing the data before training. \n",
    "\n",
    "Four parameter vectors are learning in keras' standard implementation of Batch Normalization: \n",
    "- the output scale vector (backpropagation)\n",
    "- the output offset vector (backpropagation)\n",
    "- the final input mean vector (estimated during training but used only after training)\n",
    "- the final input standard deviation vector (estimated during training but used only after training)\n",
    "\n",
    "Researchers generally found **Batch Normalization to improve all kinds of DNNs**,, e.g. lead to huge improvements on the ImageNet classification task. Using more saturating activation functions like tanh and logistic activation function become possible again. Furthermore, BN acts as a regulizer, reducing the need for dropouts.\n",
    "\n",
    "Downsides are slower predictions and slightly more complex models. Epochs are generally found to be slower, however, fewer epochs are usually needed for convergence. Thus, wall time will usually be shorter.\n",
    "\n",
    "Applying BN after each layer is so common in practice that the BN layers are hidden in diagrams. New bleeding-edge research challenges this approach but as of today it is still good practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-glucose",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BN as first layer and after every hidden layer\n",
    "# usually necessary for way deeper networks\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28])\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation_function=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-federal",
   "metadata": {},
   "source": [
    "### Gradient Clipping\n",
    "Another approach to avoid exploding gradients is to clip gradients during backpropagation by using a threshold. This approach is an alternative to BN in RNNs, for other types BN is sufficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "artificial-regulation",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-494a53b19305>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclipvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# orientation of the gradient might change; instead clipnorm=1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mse\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.SGD(clipvalue=1.0) # orientation of the gradient might change; instead clipnorm=1.0\n",
    "model.compile(loss=\"mse\", optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seeing-might",
   "metadata": {},
   "source": [
    "# 2. Reusing Pretrained Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extreme-niger",
   "metadata": {},
   "source": [
    "Chapter 14 shows ways of finding a good existing neural network. We shouldn't train a deep network from scratch but instead reuse the lower layers of an existing network. This technique is called **transfer learning**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
