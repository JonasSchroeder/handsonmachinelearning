{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "modified-first",
   "metadata": {},
   "source": [
    "# Training Deep Neural Networks (DNNs)\n",
    "To tackle complex problems we often need deeper models with 10+ hidde layers, hundreds of neurons per layer, linked by hundreds of thousands of connections.This can cause some common challenges:\n",
    "- Vanishing Gradients Problem -> many solutions below\n",
    "- Exploding Gradients Problem -> many solutions below\n",
    "- Lack of Training Data and Overfitting Noisy Data -> transfer learning and unsupervised pretraining\n",
    "- Slow Training -> choice of optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-restriction",
   "metadata": {},
   "source": [
    "## 1. Vanishing and Exploding Gradients\n",
    "The backpropagation algorithm works by going from the output layer to the input layer and computing the gradient of the cost function with regard to each parameter of the network, then using these gradients to update the parameter with Gradient Descent.\n",
    "\n",
    "As the algorithm progresses, gradients often get smaller down to the lower layers and Gradient Descent barely updates the weights of lower layers. Training never converges to a good solution. This is called the **vanishing gradients problem**. In some cases, the opposite can happen and gradients become larger and larger, leading the algorithm to diverge. This problem is called **exploding gradients** and is quite common for recurrent neural networks (RNNs).\n",
    "\n",
    "Researchers in 2010 found the combination of logistic sigmoid activation function and a normal-distribution weight initialization technique to be a common reason for this behavior. The result is that the variance of each layer's output is larger than for its input.\n",
    "\n",
    "#### Choice of Initialization\n",
    "Glorot and He propose an initialization techniques where input and output variance stays equal when moving forward (like a chain of microphones and amplifiers), and the variance of the gradients to stay equal before and after moving through the layers in backpropagation. Both cannot be guaranteed but the **Xavier or Glorot Initialization** found a good compromise that works well in practice. It is very similar to LeCun Initialization (1990s).\n",
    "\n",
    "Keras uses Glorot initialization with a uniform distribution by default. This behavior can be changed using kernal_initialization.\n",
    "\n",
    "#### Nonsaturating Activation Functions\n",
    "Next to initialization it was found that the choice of activation functions can lead to saturations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-photograph",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
